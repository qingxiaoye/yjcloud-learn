一、
1）MySQL：基本的增删改查

2）python：利用python实现数据库的增删改查
python3：
import pymysql
db = pymysql.connect(host = "",port= '',user= '',passwd= '',db = '')
cursor = db.cursor
sql = ''' '''
cursor.excute(sql)
result = cursor.fetchall()
print(result)
cursor.close()

3）什么是过拟合？怎么防止过拟合？

4）LR需要调的参数

5）LGB有哪些参数要调？作用是什么？

6）调参方法？优缺点

7）聚类方法有哪些？K均值怎么确定K?

8）gbdt和xgb的区别？

9）上面交给你一个任务，你怎么开展？

二、
1）为什么不用xgb、lgb等？

2）ngram做法

3）SVM原理

4）分词方法

5）CNN、RNN、LSTM、tensorflow框架

6）python中列表、元组、字典的不同

7）蚁群算法、退火算法（调度算法）

8）mysql中外键、主键

9）mysql性能

10）数据结构

三、
1）python中列表和元组之间的区别？
  列表可变，可增加减少；元组不可变。

2）字典是有序还是无序？从字典中取Key值，结果是什么样？
  dic = {'A': 1, 'B': 2, 'C': 3, 'F': 3, 'D': '4'}
  print dic.keys()
  >>['A', 'C', 'B', 'D', 'F']
  输出结果无序
  
22)列表求交集
   利用for循环，逐个遍历进行判断
   
  
3）生成器（一边循环一边计算，不浪费内存，保存的是算法）
   就列表而言，元素多时，非常占用内存，若只访问前几个元素，后面的均为浪费；
   
   迭代器（可以被next()函数调用并不断返回下一个值的对象称为迭代器：Iterator）
   
   一）最简单的：列表生成器  
   >> list = (x * x for x in range(10)) 
   >> next(list)  #，多次执行此句，按照顺序，一次打印出来一个，当抛出StopIteration的错误时，即穷尽了所有元素
   
   二）只要含有yield关键字的函数都是生成器函数
       yield不能和return公用
	   yield必须写在函数内部
	   return返回之后函数直接结束，而yield返回之后函数并不会结束。
   >>def generator():
		print(1)
		yield 'a'
		print(2)
		yield 'b'

   >> # 生成器函数：执行之后会得到一个生成器作为返回值
   >> res = generator()
   >> # print(res)   # <generator object generator at 0x010B0870>
   >> print(res.next())  # 生成器也即一个迭代器，所以有__next__和__iner__方法，输出的值为1 a
   >> print(res.next())
   

4）python中dataframe合并 
   按列合并 append
   按行合并 merge   （按照两个表的id）

5）python中深拷贝和浅拷贝
   注：当需要数据共享时使用浅拷贝，数据独立不进行共享时使用深拷贝

   首先，我们知道Python3中，有6个标准的数据类型，他们又分为可变和不可变。
   不可变：Number（数字）、String（字符串）、Tuple（元组）。
   可以变：List（列表）、Dictionary（字典）、Set（集合）。
   
   浅拷贝后，改变原始对象中为可变类型的元素的值，会同时影响拷贝对象；改变原始对象中为不可变类型的元素的值，不会响拷贝对象。
   深拷贝，除了顶层拷贝，还对子元素也进行了拷贝。经过深拷贝后，原始对象和拷贝对象所有的元素地址都没有相同的了。
   
   1）深浅拷贝都是对源对象的复制，占用不同的内存空间。

   2）不可变类型的对象，对于深浅拷贝毫无影响，最终的地址值和值都是相等的。

   3）可变类型：
      浅拷贝： 值相等，地址相等
      copy浅拷贝：改变原始对象中为可变类型的元素的值，会同时影响拷贝对象；改变原始对象中为不可变类型的元素的值，不会响拷贝对象
      deepcopy深拷贝：原始对象和拷贝对象所有的元素地址都没有相同的了


6）GBDT-XGB-LGB 之间的不同之处？XGB-LGB叶子生长策略有什么不同？
   相同点：都是集成算法，boosting
   不同点：
   1）gbdt

7）决策树的原理，分枝原理？

8）随机森林的随机性体现在哪里？
   1）基学习器训练样本的随机性
   2）基学期训练属性的随机性

9）逻辑斯蒂原理，需要调什么参数？
   1）penalty参数可选择的值为"L1"和"L2"，L1能使模型系数稀疏化，默认是L2的正则化。
      L2即可解决过拟合，若选择L2还是过拟合或者效果差，考虑使用L1
	  
   2）solver参数（决定算是函数的优化方法）选择受penalty影响，默认‘liblinear’；
      对于小数据集来说，“liblinear”是个不错的选择，而“sag”和'saga'对于大型数据集会更快。
      对于多类问题，只有'newton-cg'， 'sag'， 'saga'和'lbfgs'可以处理多项损失;“liblinear”仅限于“one-versus-rest”分类。
	  {'newton-cg'：基于牛顿迭代法，
	   ‘lbfgs’：基于牛顿
	   ‘liblinear’, 基于坐标下降法，
	   ‘sag’, 随机平均梯度下降
	   ‘saga’}
	   从上面的描述可以看出，newton-cg, lbfgs和sag这三种优化算法时都需要损失函数的一阶或者二阶连续导数，
	   因此不能用L1正则化，只能用于L2正则化。而liblinear通吃L1正则化和L2正则化。
	   
	3）multi_class参数（决定分类方式）
	{'ovr',默认取值，将所有K类作为正类，除此之外的所有类作为负类；liblinear，newton-cg, lbfgs和sag均可使用；
     'multinomial'：即MVM，特例one-vs-one(OvO)，任意两类进行二元逻辑回归，一共需要T(T-1)/2次分类；只能选择newton-cg, lbfgs和sag；
	}
	
	4）class_weight参数（决定各类权重，解决样本不平衡、误分类代价），默认为None，即所有类别权重一样；
	   若设置为balanced,让算法自行计算设置每一类的权重；某类样本越多，权重越低
	
	5）sample_weight(同样解决样本不平衡问题)，调用fit函数时，通过sample_weight来自己调节每个样本权重。

10）stacking融合方法原理，除了stacking还知道别的什么融合方法？
    原理：两层训练，第一层训练基于第一层训练的结果再次训练，将第一层模型预测结果作为第二层模型的训练集
	例如：
	1）第一层模型：rf、gbdt、xgb、lgb，
	  以rf为例，利用kfold将训练集train_data拆分k份，利用k-1份进行训练模型，预测第k份和测试集test_data，训练k次，得到该模型下训练集train_data的预测结果和k个test_data预测结果，然后对后者取平均；
	  同理，每个模型都可以得到对应的train_data预测结果和test_data预测结果的平均值
	2）第二层 Logistic
	  以第一层训练结果中的4个train_data预测结果作为logistic模型训练集的特征，train_data的真实标签作为标签，
	  四个模型的test_data的平均值作为测试集的特征，利用logisti进行预测。
	  
	其他融合模型：blending
	原理：比stacking简单，省去kfold部分
	1）第一层模型：rf、gbdt、xgb、lgb，
	   直接划分train_data中的一部分作为训练测试集data，训练部分为re_data，
	   四个模型分别对re_data进行训练，然后预测data和test_data，
	2）第二层 Logistic
	   data的四列预测结果作为第二层模型特征，data的真实标签作为标签，
	   四个模型预测的test_data作为测试集特征，利用logistic进行预测
	   
	
11）模型评价指标有哪些？AUC、ROC、KS、F1？召回率是什么？
    查准率（精确率）
	查全率（召回率）
	F1是查准率和查全率的加权值
	ROC曲线横坐标是假正例率，纵坐标是真正例率
	AUC是ROC曲线下方面积，用来评估模型分类好坏
	KS类似于 ROC曲线，纵坐标是假正例率、真正例率，横坐标是阈值；模型风险区分能力进行评估，KS指标（真正例率-假正例率）越大，那么模型的风险区分能力越强。
	

12）特征分箱（将连续特征离散化）
    有监督分箱：卡方分箱
	无监督分箱：等频 保证每个箱子里的数据个数相同，也是先排序
				等距 {(max-min)/N}，先排序
			    例如：[4，32，56，19，26，47]  等距分箱(N=3)结果[0, 1, 2, 1, 1, 2]；等频分箱结果[0, 1, 2, 0, 1, 2]


13）怎么求两个变量之间相关性？（数值型、类别型）
	相关性分析：相关分析：
						简单相关分析
						偏相关分析
						距离相关分析
				方差分析：
						单因素方差分析
						多因素方差分析
						协方差分析
				列联分析：
						列联表
						卡方检验

    数值型 相关关系的描述与测度：
		散点图、皮尔逊相关系数（取值在-1到1之间，小于零表示负相关，大于零表示正相关
						  取0，指标是两个变量之间不存在线性相关关系，并不说明没有任何关系，可能有非线性关系。
						  绝对值大于0.8时，说明高度相关；0.5-0.8之间，说明中度相关；
						  0.3-0.5之间，说明低度相关；小于0.3，说明相关度极弱，可视为不想关。）
	类别型：
		类别型对数值型的相关性：方差分析  例如：婚姻状况对基本费用的相关性
		类别型类别型的相关性：列联分析  例如：教育水平对客户流失的相关性

14）如果变量之间有严重相关性，对模型会产生什么影响？怎么检测变量之间多重共线性？怎么解决多重共线性？
	下列情况，暗示存在多重共线性：
			1）模型中各对自变量之间显著相关；
			2）当模型的线性关系检验显著时，几乎所有回归系数的t检验却不显著
			3）回归系数的正负号与预期的相反
    影响：模型评估失真或者不准确
	产生原因：1）解释变量有共同时间趋势；2）收集的数据不够宽；3）某些解释变量之间存在某种近似的相似关系
	检测：VIF = 1/(1 - R**2)，VIF越大，多重共线性越严重。一般认为VIF大于10时，存在严重的多重共线性。
	结果方法：1）增加样本量；2）逐步回归等方法，剔除共线变量；3）岭回归；4）主成分分析
	使用准则：1）多重共线性普遍存在，不严重即可忽略；2）影响系数符号，需要处理；3）模型仅用于预测，只要模型结果好，可忽略不处理

15）怎么判断模型的稳定程度？是否了解PSI 
    PSI: 衡量模型的预测值与实际值偏差大小的指标,用于判断模型是否适用于开发样本之外的族群
	PSI = SUM[(实际占比-预期占比)*ln(实际占比/预期占比)]
			若小于10%，无需更新模型
			10%-25%，检查一下其他度量方式
			大于25%，需要更换模型

16）python中继承和多态
    继承：子类拥有父类所有公有属性和公有方法：，还可以新增功能，还可以覆盖掉父类中不好的方法。对于父类的私有属性，子类不可以访问。对于多继承
          多个父类的有相同的某个属性，子类只继承第一个父类的属性。
	多态：相同方法在不同类中，计算结果不同
	
17）python中的pipeline 管道
    pipeline 实现了对全部步骤的流式化封装和管理，可以很方便地使参数集在新数据集上被重复使用。
	例如：
	from sklearn.preprocessing import StandardScaler
	from sklearn.decomposition import PCA
	from sklearn.linear_model import LogisticRegression
	from sklearn.pipeline import Pipeline

	pipe_lr = Pipeline([('sc', StandardScaler()),
						('pca', PCA(n_components=2)),
						('clf', LogisticRegression(random_state=1))
						])
	pipe_lr.fit(X_train, y_train)
	print('Test accuracy: %.3f' % pipe_lr.score(X_test, y_test))








































































